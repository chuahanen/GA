{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Project 4: Web Scraping Job Postings\n",
    "\n",
    "## Business Case Overview\n",
    "\n",
    "You're working as a data scientist for a contracting firm that's rapidly expanding. Now that they have their most valuable employee (you!), they need to leverage data to win more contracts. Your firm offers technology and scientific solutions and wants to be competitive in the hiring market. Your principal has two main objectives:\n",
    "\n",
    "   1. Determine the industry factors that are most important in predicting the salary amounts for these data.\n",
    "   2. Determine the factors that distinguish job categories and titles from each other. For example, can required skills accurately predict job title?\n",
    "\n",
    "To limit the scope, your principal has suggested that you *focus on data-related job postings*, e.g. data scientist, data analyst, research scientist, business intelligence, and any others you might think of. You may also want to decrease the scope by *limiting your search to a single region.*\n",
    "\n",
    "Hint: Aggregators like [Indeed.com](https://www.indeed.com) regularly pool job postings from a variety of markets and industries. \n",
    "\n",
    "**Goal:** Scrape your own data from a job aggregation tool like Indeed.com in order to collect the data to best answer these two questions.\n",
    "\n",
    "---\n",
    "\n",
    "## Directions\n",
    "\n",
    "In this project you will be leveraging a variety of skills. The first will be to use the web-scraping and/or API techniques you've learned to collect data on data jobs from Indeed.com or another aggregator. Once you have collected and cleaned the data, you will use it to answer the two questions described above.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Requirements\n",
    "\n",
    "1. Scrape and prepare your own data.\n",
    "\n",
    "2. **Create and compare at least two models for each section**. One of the two models should be a decision tree or ensemble model. The other can be a classifier or regression of your choosing (e.g. Ridge, logistic regression, KNN, SVM, etc).\n",
    "   - Section 1: Job Salary Trends\n",
    "   - Section 2: Job Category Factors\n",
    "\n",
    "3. Prepare a polished Jupyter Notebook with your analysis for a peer audience of data scientists. \n",
    "   - Make sure to clearly describe and label each section.\n",
    "   - Comment on your code so that others could, in theory, replicate your work.\n",
    "\n",
    "4. A brief writeup in an executive summary, written for a non-technical audience.\n",
    "   - Writeups should be at least 500-1000 words, defining any technical terms, explaining your approach, as well as any risks and limitations.\n",
    "\n",
    "#### BONUS\n",
    "\n",
    "5. Answer the salary discussion by using your model to explain the tradeoffs between detecting high vs low salary positions.\n",
    "\n",
    "6. Convert your executive summary into a public blog post of at least 500 words, in which you document your approach in a tutorial for other aspiring data scientists. Link to this in your notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Suggestions for Getting Started\n",
    "\n",
    "1. Collect data from [Indeed.com](www.indeed.com) (or another aggregator) on data-related jobs to use in predicting salary trends for your analysis.\n",
    "  - Select and parse data from *at least 1000 postings* for jobs, potentially from multiple location searches.\n",
    "2. Find out what factors most directly impact salaries (e.g. title, location, department, etc).\n",
    "  - Test, validate, and describe your models. What factors predict salary category? How do your models perform?\n",
    "3. Discover which features have the greatest importance when determining a low vs. high paying job.\n",
    "  - Your Boss is interested in what overall features hold the greatest significance.\n",
    "  - HR is interested in which SKILLS and KEY WORDS hold the greatest significance.   \n",
    "4. Author an executive summary that details the highlights of your analysis for a non-technical audience.\n",
    "5. If tackling the bonus question, try framing the salary problem as a classification problem detecting low vs. high salary positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Users/Han/Downloads/Data/Git/GA/chromedriver/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(executable_path=chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'https://www.indeed.com/jobs?q=Data+Scientist&l=United+States&sort=date'\n",
    "url_130k = 'https://www.indeed.com/q-Data-Scientist-$130,000-l-United-States-jobs.html'\n",
    "driver.get(url_130k)\n",
    "sleep(3)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = pd.DataFrame(columns=['title','company','location','desc','date','salary','estimate','desired'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlinks_indeed():\n",
    "    for row in soup.findAll(\"a\", {\"class\" : \"turnstileLink\"}):\n",
    "        if \"/cmp/\" not in row['href']:\n",
    "            links.append(row['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getinfo_indeed():\n",
    "    #convert to css format\n",
    "    css_selector = \"a[href*=\"+'\"'+href+'\"'\n",
    "    driver.find_element_by_css_selector(css_selector).click()\n",
    "    sleep(1.5)\n",
    "\n",
    "    soup1 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    title = soup1.find(\"div\", {\"id\" : \"vjs-jobinfo\"}).find(\"div\", {\"id\" : \"vjs-jobtitle\"}).text\n",
    "    company = soup1.find(\"span\", {\"id\" : \"vjs-cn\"}).text\n",
    "    location = soup1.find(\"div\", {\"id\" : \"vjs-jobinfo\"}).find(\"span\", {\"id\":\"vjs-loc\"}).text\n",
    "#     try:\n",
    "#         rating = soup1.find(\"div\", {\"id\" : \"vjs-jobinfo\"}).find(\"span\", {\"class\":\"rating\"})['style']\n",
    "#         reviews = soup1.find(\"div\", {\"id\" : \"vjs-jobinfo\"}).find(\"span\", {\"class\":\"slNoUnderline\"}).text\n",
    "#     except TypeError:\n",
    "#         rating = 0\n",
    "#         reviews = 0\n",
    "    desc = soup1.find(\"div\", {\"id\" : \"vjs-desc\"}).text\n",
    "    date = soup1.find(\"span\", {\"class\" : \"date\"}).text\n",
    "    s_test = (soup1.find(\"div\", {\"id\" : \"vjs-jobinfo\"}).find_all('span'))\n",
    "    if True in ['$' in x.text for x in s_test]:\n",
    "        salary = s_test[['$' in x.text for x in s_test].index(True)].text\n",
    "    else:\n",
    "        salary = -1\n",
    "    \n",
    "    estimate = '$13000+' #additional column for jobs using salary estimates\n",
    "    desired = []\n",
    "    for skills in soup1.find('div', {'id':'vjs-container'}).findAll('span', {'class':'experienceListItem'}):\n",
    "        desired.append(skills.text)\n",
    "\n",
    "    scrape.loc[len(scrape)] = [title, company, location, desc, date, salary, estimate, desired]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try again function\n",
    "def try_again():\n",
    "    last_page = scrape[scrape['title'] == 'Next page'].index.max()\n",
    "    scrape.drop(index=scrape.index[last_page+1:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pages in range(50):\n",
    "    sleep(50)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    links = []\n",
    "    getlinks_indeed()\n",
    "    for href in links:\n",
    "        try:\n",
    "            getinfo_indeed()\n",
    "        \n",
    "        except WebDriverException:\n",
    "            # handles popups\n",
    "            driver.find_element_by_class_name('popover-x').click()\n",
    "            getinfo_indeed()\n",
    "            \n",
    "        except:\n",
    "            # handles incomplete loading\n",
    "            sleep(5)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            try_again()\n",
    "            links = []\n",
    "            getlinks_indeed()\n",
    "            for href in links:\n",
    "                getinfo_indeed()\n",
    "    \n",
    "    #go to next page\n",
    "    driver.find_elements_by_class_name('pn')[-1].click()\n",
    "    scrape.loc[len(scrape)] = ['Next page']*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>desc</th>\n",
       "      <th>date</th>\n",
       "      <th>salary</th>\n",
       "      <th>estimate</th>\n",
       "      <th>desired</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>Data Scientist - Machine Learning Engineer</td>\n",
       "      <td>Aera Technology</td>\n",
       "      <td>- Mountain View, CA 94041</td>\n",
       "      <td>Do you want to shape the future of enterprise ...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[TensorFlow, Linux, Spark, Machine Learning, K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>Program Manager, Machine Learning Data Ecosyst...</td>\n",
       "      <td>Google</td>\n",
       "      <td>- Mountain View, CA</td>\n",
       "      <td>Google's projects, like our users, span the gl...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Machine Learning, Project Management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>Scientist, High Impact Data- Protein Analytics...</td>\n",
       "      <td>Bristol-Myers Squibb</td>\n",
       "      <td>- Redwood City, CA</td>\n",
       "      <td>High Impact Data Scientist for Protein Analyti...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Spectroscopy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>Machine Learning Engineer</td>\n",
       "      <td>Dropbox</td>\n",
       "      <td>- San Francisco, CA</td>\n",
       "      <td>----------------------\\nRole Description\\n----...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[TensorFlow, AI, Machine Learning, Image Proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>Data Scientist - Machine Learning</td>\n",
       "      <td>Affirm</td>\n",
       "      <td>- San Francisco, CA 94126 (Financial District...</td>\n",
       "      <td>What You'll Do\\nBuild production fraud and cre...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Machine Learning, Data Analysis, Python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>Analytics Software – Machine Learning Scientist</td>\n",
       "      <td>FICO</td>\n",
       "      <td>- San Jose, CA 95110 (Downtown area)</td>\n",
       "      <td>The Machine Learning Scientist will be a contr...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Big Data, JavaScript, Java, Spark, Machine Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Convoy</td>\n",
       "      <td>- Seattle, WA</td>\n",
       "      <td>Convoy, one of the fastest growing startups in...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Big Data, Spark, Machine Learning, Hadoop, R,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>LeapYear</td>\n",
       "      <td>- Berkeley, CA</td>\n",
       "      <td>As a data scientist at LeapYear, you will be r...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[TensorFlow, Hive, Spark, Machine Learning, Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Picarro</td>\n",
       "      <td>- Santa Clara, CA 95054</td>\n",
       "      <td>The Opportunity:\\nPicarro is seeking an accomp...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[ArcGIS, Statisical Analysis, Python, GIS, SQL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Paradigm Infotech, Inc</td>\n",
       "      <td>- San Jose, CA</td>\n",
       "      <td>Role: Data ScientistLocation: San Jose CADurat...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Spark, Machine Learning, Hadoop, R, Python, SQL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1142</th>\n",
       "      <td>Analytics Software – Machine Learning Scientist</td>\n",
       "      <td>FICO</td>\n",
       "      <td>- San Jose, CA 95110 (Downtown area)</td>\n",
       "      <td>The Machine Learning Scientist will be a contr...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Big Data, JavaScript, Java, Spark, Machine Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Convoy</td>\n",
       "      <td>- Seattle, WA</td>\n",
       "      <td>Convoy, one of the fastest growing startups in...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Big Data, Spark, Machine Learning, Hadoop, R,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>LeapYear</td>\n",
       "      <td>- Berkeley, CA</td>\n",
       "      <td>As a data scientist at LeapYear, you will be r...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[TensorFlow, Hive, Spark, Machine Learning, Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Picarro</td>\n",
       "      <td>- Santa Clara, CA 95054</td>\n",
       "      <td>The Opportunity:\\nPicarro is seeking an accomp...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[ArcGIS, Statisical Analysis, Python, GIS, SQL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Paradigm Infotech, Inc</td>\n",
       "      <td>- San Jose, CA</td>\n",
       "      <td>Role: Data ScientistLocation: San Jose CADurat...</td>\n",
       "      <td>10 days ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Spark, Machine Learning, Hadoop, R, Python, SQL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Taboola</td>\n",
       "      <td>- Los Angeles, CA</td>\n",
       "      <td>Join Taboola's Big Data team as a Sr. Data Sci...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Pig, Hive, Machine Learning, Hadoop, Scala, G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Adobe</td>\n",
       "      <td>- San Jose, CA</td>\n",
       "      <td>Adobe is looking for a Data Scientist to help ...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Pig, AI, Hive, Data Mining, Machine Learning,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>Manager, Data Scientist, Personalization</td>\n",
       "      <td>Hilton Corporate</td>\n",
       "      <td>- Addison, TX</td>\n",
       "      <td>Work Locations\\n:\\nHilton - Shared Services - ...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[AI, Machine Learning, Hadoop, R, Scala, Kafka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>Data Scientist, Audience Science</td>\n",
       "      <td>Ubisoft</td>\n",
       "      <td>- San Francisco, CA 94107 (South Of Market area)</td>\n",
       "      <td>Job Description\\n\\nUbisoft, leading creator, p...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Spark, Hadoop, R, Python, SQL, Tableau]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>Data Engineer, Cloud Platforms</td>\n",
       "      <td>Tesla</td>\n",
       "      <td>- Palo Alto, CA</td>\n",
       "      <td>Role\\nData is deeply embedded in the product a...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Java, Hive, Spark, Hadoop, Kafka, Scala, Pyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Coso IT</td>\n",
       "      <td>- Washington, DC</td>\n",
       "      <td>We are looking to hire Data Scientists with at...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Pig, Data Mining, TS/SCI Clearance, Machine L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>Sr. Big Data Engineer &amp; Data Scientist</td>\n",
       "      <td>VMware</td>\n",
       "      <td>- Palo Alto, CA</td>\n",
       "      <td>Sr. Big Data Engineer &amp; Data Scientist\\n\\nYour...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Pig, Hive, Data Mining, Machine Learning, VMW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Applied Memetics LLC</td>\n",
       "      <td>- Arlington, VA</td>\n",
       "      <td>Overview\\nAM LLC is seeking a data scientist w...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Machine Learning, Hadoop, R, Scala, Kafka, Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Integral Ad Science</td>\n",
       "      <td>- New York, NY 10003 (Greenwich Village area)</td>\n",
       "      <td>We are seeking a Data Scientist to join our te...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[AI, Hive, Spark, Machine Learning, Hadoop, Py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>Data Scientist, Analytics (Integrity)</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>- Menlo Park, CA</td>\n",
       "      <td>Facebook's mission is to give people the power...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Hive, Quantitative Analysis, Hadoop, R, Oracl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>Mid-Level Business Intelligence &amp; Data Archite...</td>\n",
       "      <td>FAR Solution</td>\n",
       "      <td>- Fort Meade, MD</td>\n",
       "      <td>Telecommute: All work to be performed on the g...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[Hive, Machine Learning, Data Management, R, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>Software Engineer, Data Engineering/Machine Le...</td>\n",
       "      <td>Niantic Labs</td>\n",
       "      <td>- San Francisco, CA</td>\n",
       "      <td>Niantic’s Engineering Team is seeking an exper...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[TensorFlow, AI, Data Mining, Machine Learning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>2019 Business Management Analyst 2 (Woodland H...</td>\n",
       "      <td>Northrop Grumman</td>\n",
       "      <td>- Woodland Hills, CA 91367</td>\n",
       "      <td>2019 Business Management Analyst 2 (Woodland H...</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>-1</td>\n",
       "      <td>$13000+</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "      <td>Next page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "1132         Data Scientist - Machine Learning Engineer   \n",
       "1133  Program Manager, Machine Learning Data Ecosyst...   \n",
       "1134  Scientist, High Impact Data- Protein Analytics...   \n",
       "1135                          Machine Learning Engineer   \n",
       "1136                  Data Scientist - Machine Learning   \n",
       "1137    Analytics Software – Machine Learning Scientist   \n",
       "1138                                     Data Scientist   \n",
       "1139                                     Data Scientist   \n",
       "1140                              Senior Data Scientist   \n",
       "1141                                     Data Scientist   \n",
       "1142    Analytics Software – Machine Learning Scientist   \n",
       "1143                                     Data Scientist   \n",
       "1144                                     Data Scientist   \n",
       "1145                              Senior Data Scientist   \n",
       "1146                                     Data Scientist   \n",
       "1147                                          Next page   \n",
       "1148                                 Sr. Data Scientist   \n",
       "1149                                     Data Scientist   \n",
       "1150           Manager, Data Scientist, Personalization   \n",
       "1151                   Data Scientist, Audience Science   \n",
       "1152                     Data Engineer, Cloud Platforms   \n",
       "1153                                     Data Scientist   \n",
       "1154             Sr. Big Data Engineer & Data Scientist   \n",
       "1155                                     Data Scientist   \n",
       "1156                                     Data Scientist   \n",
       "1157              Data Scientist, Analytics (Integrity)   \n",
       "1158  Mid-Level Business Intelligence & Data Archite...   \n",
       "1159  Software Engineer, Data Engineering/Machine Le...   \n",
       "1160  2019 Business Management Analyst 2 (Woodland H...   \n",
       "1161                                          Next page   \n",
       "\n",
       "                     company  \\\n",
       "1132         Aera Technology   \n",
       "1133                  Google   \n",
       "1134    Bristol-Myers Squibb   \n",
       "1135                 Dropbox   \n",
       "1136                  Affirm   \n",
       "1137                    FICO   \n",
       "1138                  Convoy   \n",
       "1139                LeapYear   \n",
       "1140                 Picarro   \n",
       "1141  Paradigm Infotech, Inc   \n",
       "1142                    FICO   \n",
       "1143                  Convoy   \n",
       "1144                LeapYear   \n",
       "1145                 Picarro   \n",
       "1146  Paradigm Infotech, Inc   \n",
       "1147               Next page   \n",
       "1148                 Taboola   \n",
       "1149                   Adobe   \n",
       "1150        Hilton Corporate   \n",
       "1151                 Ubisoft   \n",
       "1152                   Tesla   \n",
       "1153                 Coso IT   \n",
       "1154                  VMware   \n",
       "1155    Applied Memetics LLC   \n",
       "1156     Integral Ad Science   \n",
       "1157                Facebook   \n",
       "1158            FAR Solution   \n",
       "1159            Niantic Labs   \n",
       "1160        Northrop Grumman   \n",
       "1161               Next page   \n",
       "\n",
       "                                               location  \\\n",
       "1132                          - Mountain View, CA 94041   \n",
       "1133                                - Mountain View, CA   \n",
       "1134                                 - Redwood City, CA   \n",
       "1135                                - San Francisco, CA   \n",
       "1136   - San Francisco, CA 94126 (Financial District...   \n",
       "1137               - San Jose, CA 95110 (Downtown area)   \n",
       "1138                                      - Seattle, WA   \n",
       "1139                                     - Berkeley, CA   \n",
       "1140                            - Santa Clara, CA 95054   \n",
       "1141                                     - San Jose, CA   \n",
       "1142               - San Jose, CA 95110 (Downtown area)   \n",
       "1143                                      - Seattle, WA   \n",
       "1144                                     - Berkeley, CA   \n",
       "1145                            - Santa Clara, CA 95054   \n",
       "1146                                     - San Jose, CA   \n",
       "1147                                          Next page   \n",
       "1148                                  - Los Angeles, CA   \n",
       "1149                                     - San Jose, CA   \n",
       "1150                                      - Addison, TX   \n",
       "1151   - San Francisco, CA 94107 (South Of Market area)   \n",
       "1152                                    - Palo Alto, CA   \n",
       "1153                                   - Washington, DC   \n",
       "1154                                    - Palo Alto, CA   \n",
       "1155                                    - Arlington, VA   \n",
       "1156      - New York, NY 10003 (Greenwich Village area)   \n",
       "1157                                   - Menlo Park, CA   \n",
       "1158                                   - Fort Meade, MD   \n",
       "1159                                - San Francisco, CA   \n",
       "1160                         - Woodland Hills, CA 91367   \n",
       "1161                                          Next page   \n",
       "\n",
       "                                                   desc         date  \\\n",
       "1132  Do you want to shape the future of enterprise ...  10 days ago   \n",
       "1133  Google's projects, like our users, span the gl...  10 days ago   \n",
       "1134  High Impact Data Scientist for Protein Analyti...  10 days ago   \n",
       "1135  ----------------------\\nRole Description\\n----...  10 days ago   \n",
       "1136  What You'll Do\\nBuild production fraud and cre...  10 days ago   \n",
       "1137  The Machine Learning Scientist will be a contr...  10 days ago   \n",
       "1138  Convoy, one of the fastest growing startups in...  10 days ago   \n",
       "1139  As a data scientist at LeapYear, you will be r...  10 days ago   \n",
       "1140  The Opportunity:\\nPicarro is seeking an accomp...  10 days ago   \n",
       "1141  Role: Data ScientistLocation: San Jose CADurat...  10 days ago   \n",
       "1142  The Machine Learning Scientist will be a contr...  10 days ago   \n",
       "1143  Convoy, one of the fastest growing startups in...  10 days ago   \n",
       "1144  As a data scientist at LeapYear, you will be r...  10 days ago   \n",
       "1145  The Opportunity:\\nPicarro is seeking an accomp...  10 days ago   \n",
       "1146  Role: Data ScientistLocation: San Jose CADurat...  10 days ago   \n",
       "1147                                          Next page    Next page   \n",
       "1148  Join Taboola's Big Data team as a Sr. Data Sci...   1 hour ago   \n",
       "1149  Adobe is looking for a Data Scientist to help ...   1 hour ago   \n",
       "1150  Work Locations\\n:\\nHilton - Shared Services - ...   1 hour ago   \n",
       "1151  Job Description\\n\\nUbisoft, leading creator, p...   1 hour ago   \n",
       "1152  Role\\nData is deeply embedded in the product a...   1 hour ago   \n",
       "1153  We are looking to hire Data Scientists with at...   1 hour ago   \n",
       "1154  Sr. Big Data Engineer & Data Scientist\\n\\nYour...   1 hour ago   \n",
       "1155  Overview\\nAM LLC is seeking a data scientist w...   1 hour ago   \n",
       "1156  We are seeking a Data Scientist to join our te...   1 hour ago   \n",
       "1157  Facebook's mission is to give people the power...   1 hour ago   \n",
       "1158  Telecommute: All work to be performed on the g...   1 hour ago   \n",
       "1159  Niantic’s Engineering Team is seeking an exper...   1 hour ago   \n",
       "1160  2019 Business Management Analyst 2 (Woodland H...   1 hour ago   \n",
       "1161                                          Next page    Next page   \n",
       "\n",
       "         salary   estimate                                            desired  \n",
       "1132         -1    $13000+  [TensorFlow, Linux, Spark, Machine Learning, K...  \n",
       "1133         -1    $13000+             [Machine Learning, Project Management]  \n",
       "1134         -1    $13000+                                     [Spectroscopy]  \n",
       "1135         -1    $13000+  [TensorFlow, AI, Machine Learning, Image Proce...  \n",
       "1136         -1    $13000+          [Machine Learning, Data Analysis, Python]  \n",
       "1137         -1    $13000+  [Big Data, JavaScript, Java, Spark, Machine Le...  \n",
       "1138         -1    $13000+  [Big Data, Spark, Machine Learning, Hadoop, R,...  \n",
       "1139         -1    $13000+  [TensorFlow, Hive, Spark, Machine Learning, Da...  \n",
       "1140         -1    $13000+    [ArcGIS, Statisical Analysis, Python, GIS, SQL]  \n",
       "1141         -1    $13000+  [Spark, Machine Learning, Hadoop, R, Python, SQL]  \n",
       "1142         -1    $13000+  [Big Data, JavaScript, Java, Spark, Machine Le...  \n",
       "1143         -1    $13000+  [Big Data, Spark, Machine Learning, Hadoop, R,...  \n",
       "1144         -1    $13000+  [TensorFlow, Hive, Spark, Machine Learning, Da...  \n",
       "1145         -1    $13000+    [ArcGIS, Statisical Analysis, Python, GIS, SQL]  \n",
       "1146         -1    $13000+  [Spark, Machine Learning, Hadoop, R, Python, SQL]  \n",
       "1147  Next page  Next page                                          Next page  \n",
       "1148         -1    $13000+  [Pig, Hive, Machine Learning, Hadoop, Scala, G...  \n",
       "1149         -1    $13000+  [Pig, AI, Hive, Data Mining, Machine Learning,...  \n",
       "1150         -1    $13000+  [AI, Machine Learning, Hadoop, R, Scala, Kafka...  \n",
       "1151         -1    $13000+           [Spark, Hadoop, R, Python, SQL, Tableau]  \n",
       "1152         -1    $13000+  [Java, Hive, Spark, Hadoop, Kafka, Scala, Pyth...  \n",
       "1153         -1    $13000+  [Pig, Data Mining, TS/SCI Clearance, Machine L...  \n",
       "1154         -1    $13000+  [Pig, Hive, Data Mining, Machine Learning, VMW...  \n",
       "1155         -1    $13000+  [Machine Learning, Hadoop, R, Scala, Kafka, Ja...  \n",
       "1156         -1    $13000+  [AI, Hive, Spark, Machine Learning, Hadoop, Py...  \n",
       "1157         -1    $13000+  [Hive, Quantitative Analysis, Hadoop, R, Oracl...  \n",
       "1158         -1    $13000+  [Hive, Machine Learning, Data Management, R, B...  \n",
       "1159         -1    $13000+  [TensorFlow, AI, Data Mining, Machine Learning...  \n",
       "1160         -1    $13000+                                                 []  \n",
       "1161  Next page  Next page                                          Next page  "
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_again()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 8)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "504"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scrape[scrape[['desc','company']].duplicated()].sort_values(by='desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "658"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1162-504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_unique = scrape.drop(scrape[scrape[['desc','company']].duplicated()].index)\n",
    "scrape_indeed = scrape_unique[~(scrape_unique['title'] == 'Next page')]\n",
    "\n",
    "scrape_indeed[scrape_indeed['salary'] != -1].reset_index(drop=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#60K: stopped at page 52 (505 rows)\n",
    "#95K: stopped at page 52 (473 rows)\n",
    "#130K: stopped at page ? (658 rows)\n",
    "# scrape_indeed.to_csv('/Users/Han/Downloads/scrape_indeed_60.csv')\n",
    "# scrape_indeed.to_csv('/Users/Han/Downloads/scrape_indeed_95.csv')\n",
    "scrape_indeed.to_csv('/Users/Han/Downloads/scrape_indeed_130.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## efinancialcareers.com\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Users/Han/Downloads/Data/Git/GA/chromedriver/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(executable_path=chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.efinancialcareers.com/search/?q=data%20scientist&countryCode=US&currencyCode=USD&language=en&facets=*&page=1&pageSize=10&filters.salaryBand=FIRST_TIER%7CSECOND_TIER%7CTHIRD_TIER%7CFOURTH_TIER%7CFIFTH_TIER%7CSIXTH_TIER&ds=sr'\n",
    "driver.get(url)\n",
    "sleep(3)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = pd.DataFrame(columns=['title','company','location','desc','date','salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getinfo_efc():\n",
    "    soup1 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    title = soup1.find('h1').text\n",
    "    company = soup1.find('li', {'class':'company'}).find('span').text\n",
    "    location = soup1.find('li', {'class':'location'}).find('span').text\n",
    "    location = location.splitlines()[-1].lstrip()\n",
    "    desc = soup1.find('section', {'class':'description'}).text + \\\n",
    "            soup1.find('li', {'class':'position'}).find('span').text\n",
    "    date = soup1.find('li', {'class':'updated'}).find('span').text\n",
    "    salary = soup1.find('li', {'class':'salary'}).find('span').text\n",
    "\n",
    "    scrape.loc[len(scrape)] = [title, company, location, desc, date, salary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# various types of formatting for special jobs/companies\n",
    "def getinfo_efc_alt():\n",
    "    soup1 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    title = soup1.find(['h1','h2']).text\n",
    "    company = soup1.findAll('h2')[1].text\n",
    "    company = company.splitlines()[-1].lstrip()\n",
    "    try:\n",
    "        location = soup1.find('div', {'class':'detailsvisible'}).findAll('li')[0].text\n",
    "        location = location.split(':')[-1].lstrip()\n",
    "    except:\n",
    "        location = 0\n",
    "    \n",
    "    desc = soup1.find(['div','section'], {'class':['description','job-description']}).text \n",
    "       \n",
    "    try:\n",
    "        salary = soup1.find('div', {'class':'detailsvisible'}).findAll('li')[1].text\n",
    "        salary = salary.split(':')[-1].lstrip()\n",
    "    except:\n",
    "        salary = 0\n",
    "    \n",
    "    date = 0\n",
    "    \n",
    "    scrape.loc[len(scrape)] = [title, company, location, desc, date, salary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resume scraping from last left off page\n",
    "#url = 'https://www.efinancialcareers.com/search/?q=data%20scientist&countryCode=US&currencyCode=USD&language=en&facets=*&page=6&pageSize=10&filters.salaryBand=FIRST_TIER%7CSECOND_TIER%7CTHIRD_TIER%7CFOURTH_TIER%7CFIFTH_TIER%7CSIXTH_TIER&ds=sr'\n",
    "#driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pages in range(50):\n",
    "    sleep(5)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    for link in driver.find_elements_by_class_name('card-title-link'):\n",
    "        try:\n",
    "            link.click()\n",
    "        \n",
    "        #to handle (ignore) pop-ups\n",
    "        except WebDriverException:\n",
    "            driver.switch_to.active_element.click()\n",
    "\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        sleep(5)\n",
    "        \n",
    "        try: #scrape\n",
    "            getinfo_efc()\n",
    "            driver.close()\n",
    "            \n",
    "        except AttributeError: \n",
    "            soup1 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "            if soup1.find('h2').text != None:\n",
    "                getinfo_efc_alt() # to handle alternative formatting for special jobs/companies\n",
    "                driver.close()\n",
    "                \n",
    "            else:\n",
    "                sleep(5)\n",
    "                getinfo_efc()\n",
    "                driver.close()\n",
    "            \n",
    "        #switch back to main page\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        sleep(0.5)\n",
    "\n",
    "    #go to next page\n",
    "    driver.find_element_by_id('searchPaginationNext-li').click()\n",
    "    scrape.loc[len(scrape)] = ['Next page']*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# soup1 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# title = soup1.find('h2').text\n",
    "# company = soup1.findAll('h2')[1].text\n",
    "# company = company.splitlines()[-1].lstrip()\n",
    "# location = soup1.find('div', {'class':'detailsvisible'}).findAll('li')[0].text\n",
    "# location = location.split(':')[-1].lstrip()\n",
    "\n",
    "\n",
    "# rating = 0\n",
    "# reviews = 0\n",
    "# desc = soup1.find('section', {'class':'description'}).text + \\\n",
    "#         soup1.find('div', {'class':'detailsvisible'}).findAll('li')[2].text\n",
    "# date = 0\n",
    "# salary = soup1.find('div', {'class':'detailsvisible'}).findAll('li')[1].text\n",
    "# salary = salary.split(':')[-1].lstrip()\n",
    "\n",
    "# test = pd.DataFrame(columns=['title','company','location','rating','reviews','desc','date','salary'])\n",
    "# test.loc[len(test)] = [title, company, location, rating, reviews, desc, date, salary]\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "\n",
    "# #switch back to main page\n",
    "# driver.switch_to.window(driver.window_handles[0])\n",
    "# sleep(0.5)\n",
    "\n",
    "# try_again()\n",
    "# scrape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_unique = scrape.drop(scrape[scrape[['desc','company']].duplicated()].index)\n",
    "scrape_efc = scrape_unique[~(scrape_unique['title'] == 'Next page')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150     £excellent + bonus + good package\n",
       "151                                 $High\n",
       "152    up to $240,000 plus bonus and bens\n",
       "153         GBP63000 - GBP73000 per annum\n",
       "154      £50,000 - £90,000 base + package\n",
       "155                               £55,000\n",
       "156         GBP55707 - GBP75707 per annum\n",
       "157              Up to GBP60000 per annum\n",
       "158                           Competitive\n",
       "159                                     0\n",
       "Name: salary, dtype: object"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_efc['salary'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x,y) = (scrape_efc.shape)\n",
    "x-sum(scrape['salary'].value_counts().head().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scrape_efc.to_csv('/Users/Han/Downloads/scrape_efc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last website...\n",
    "</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Users/Han/Downloads/Data/Git/GA/chromedriver/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(executable_path=chromedriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.simplyhired.com/search?q=data+scientist&l=united+states&mip=%2460%2C000&pp=&'\n",
    "driver.get(url)\n",
    "sleep(3)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = pd.DataFrame(columns=['title','company','location','desc','date','salary','estimate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlinks_simplyhired():\n",
    "    for row in soup.findAll(\"a\", {\"class\" : \"card-link\"}):\n",
    "         links.append(\"https://www.simplyhired.com\"+row['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getinfo_simplyhired(i):\n",
    "    soup1 = BeautifulSoup(driver1.page_source, 'lxml')\n",
    "\n",
    "    title = soup1.find('h1', {'itemprop' : 'title'}).text\n",
    "    company = soup1.find('span', {'class':'company'}).text\n",
    "    location = soup1.find('span', {'class':'location'}).text\n",
    "    desc = soup1.find('div', {'class':'viewjob-description'}).text\n",
    "    date = soup.findAll('span', {'class':'jobposting-timestamp'})[i].find('time')['datetime']\n",
    "    salary = soup.findAll('span', {'class':'jobposting-salary'})[i]['data-salary']\n",
    "    estimate = soup.findAll('span', {'class':'jobposting-salary'})[i]['data-est']\n",
    "\n",
    "    scrape.loc[len(scrape)] = [title, company, location, desc, date, salary, estimate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try again function\n",
    "def try_again():\n",
    "        last_page = scrape[scrape['title'] == 'Next page'].index.max()\n",
    "        scrape.drop(index=scrape.index[last_page+1:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pages in range(100):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    links = []\n",
    "    getlinks_simplyhired()\n",
    "    for i, job in enumerate(links):\n",
    "        driver1 = webdriver.Chrome(executable_path=chromedriver)\n",
    "        driver1.get(job)\n",
    "        sleep(3)\n",
    "    \n",
    "        getinfo_simplyhired(i)\n",
    "        driver1.close()\n",
    "  \n",
    "    #go to next page\n",
    "    driver.find_element_by_class_name('next-pagination').click()\n",
    "    scrape.loc[len(scrape)] = ['Next page']*7\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1042, 7)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scrape[scrape[['desc','company']].duplicated()].sort_values(by='desc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(964, 7)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_unique = scrape.drop(scrape[scrape[['desc','company']].duplicated()].index)\n",
    "scrape_simplyhired = scrape_unique[~(scrape_unique['title'] == 'Next page')]\n",
    "\n",
    "scrape_simplyhired.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 7)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_simplyhired[scrape_simplyhired['estimate'] == 'false'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopped at page 50 (964 rows)\n",
    "scrape_simplyhired.to_csv('/Users/Han/Downloads/scrape_simplyhired.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_again()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, job in enumerate(links[:2]):\n",
    "#     chromedriver = \"/Users/Han/Downloads/Data/Git/GA/chromedriver/chromedriver\"\n",
    "#     os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "#     driver1 = webdriver.Chrome(executable_path=chromedriver)\n",
    "#     driver1.get(job)\n",
    "#     sleep(3)\n",
    "    \n",
    "#     soup1 = BeautifulSoup(driver1.page_source, 'lxml')\n",
    "#     print(soup1.find('h1', {'itemprop' : 'title'}).text)\n",
    "#     print(soup1.find('span', {'class':'company'}).text)\n",
    "#     print(soup1.find('span', {'class':'location'}).text)\n",
    "#     print(soup1.find('div', {'class':'viewjob-description'}).text)\n",
    "    \n",
    "#     print(soup.findAll('span', {'class':'jobposting-salary'})[i]['data-salary'])\n",
    "#     print(soup.findAll('span', {'class':'jobposting-salary'})[i]['data-est'])\n",
    "#     print(soup.findAll('span', {'class':'jobposting-timestamp'})[i].find('time')['datetime'])\n",
    "#     driver1.close()\n",
    "#     print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1: Factors that impact salary\n",
    "\n",
    "To predict salary you will be building either a classification or regression model, using features like the location, title, and summary of the job. If framing this as a regression problem, you will be estimating the listed salary amounts. You may instead choose to frame this as a classification problem, in which case you will create labels from these salaries (high vs. low salary, for example) according to thresholds (such as median salary).\n",
    "\n",
    "You have learned a variety of new skills and models that may be useful for this problem:\n",
    "- NLP\n",
    "- Unsupervised learning and dimensionality reduction techniques (PCA, clustering)\n",
    "- Ensemble methods and decision tree models\n",
    "- SVM models\n",
    "\n",
    "Whatever you decide to use, the most important thing is to justify your choices and interpret your results. *Communication of your process is key.* Note that most listings **DO NOT** come with salary information. You'll need to able to extrapolate or predict the expected salaries for these listings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 2: Factors that distinguish job category\n",
    "\n",
    "Using the job postings you scraped for part 1 (or potentially new job postings from a second round of scraping), identify features in the data related to job postings that can distinguish job titles from each other. There are a variety of interesting ways you can frame the target variable, for example:\n",
    "- What components of a job posting distinguish data scientists from other data jobs?\n",
    "- What features are important for distinguishing junior vs. senior positions?\n",
    "- Do the requirements for titles vary significantly with industry (e.g. healthcare vs. government)?\n",
    "\n",
    "You may end up making multiple classification models to tackle different questions. Be sure to clearly explain your hypotheses and framing, any feature engineering, and what your target variables are. The type of classification model you choose is up to you. Be sure to interpret your results and evaluate your models' performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS PROBLEM\n",
    "\n",
    "Your boss would rather tell a client incorrectly that they would get a lower salary job than tell a client incorrectly that they would get a high salary job. Adjust one of your models to ease his mind, and explain what it is doing and any tradeoffs. Plot the ROC curve.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
